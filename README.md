# SOSHO - Rule Based Artificial General Intelligence

## Introduction

Welcome to the SOSHO! Our goal is to explore the possibilities of Rule-Based Artificial General Intelligence (RAGI) and develop a baby AGI that can learn from and interact with humans. This project is all about pushing the boundaries of what is currently possible with RAGI and building a foundation for more advanced AGI systems in the future.

The parts of cognition that we are trying to emulate are:
- THINKING
- KNOWING
- REMEMBERING
- JUDGING
- PROBLEM SOLVING


## Development

We are building SOSHO using a combination of rule-based and machine learning approaches. Our development stack includes tools like [H2O](https://github.com/h2oai/h2o-3) with [Studio](https://github.com/h2oai/h2o-llmstudio) for [AutoML](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html), and several UI options to make our AGI more user-friendly. We are leveraging rule-based techniques to develop an initial set of rules that our AGI can use to interact with humans and learn from them. We are also using machine learning to identify patterns in human interactions and to refine our set of rules.



## Architecture

### Passive exploration
As we develop SOSHO, we want to be able to gain insights into the inner workings of our baby AGI. One way to do this is by understanding the embeddings of all the text that flows through our conversations. We can start to build a complex knowledge graph that goes beyond just text. This can be achieved using tools like the [Atlas](https://github.com/nomic-ai/nomic) Visualizer, which allows us to explore, label, and search the embeddings generated by our model. By doing this, we can gain a deeper understanding of how SOSHO is processing and storing information, which will help us continue to improve and develop our baby AGI.


### Chat thread (Step-by-Step)
- STEP 1 - Check if any callbacks are awaiting to be resolved (if yes: display them in the chat)

- OPTIONAL
    - Speech recognition using [whisper](https://github.com/openai/whisper) (voice->text)
    - Speech genertion using [Bark](https://huggingface.co/spaces/suno/bark) (text->voice)

- STEP 2 - Initialize conversation
    - Ask what task to do
    - Provide suggessions for tasks that can be done (self reflection)
    - Accept natural Language input from user (with image inputs)

- STEP 3 - Intent recognition from natural language input
    - INTENT 1 - Act as a prompt generator (Can be used as a prompt generator for INTENT 2)
        - Generate what prompt to use to generate a similar iamge [(CLIP-Interrogator)](https://huggingface.co/spaces/pharma/CLIP-Interrogator)
        - [Stable diff prompt generator](https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion)
    - INTENT 2 - Using the model's pretrained information, describe a word / phrase / answer general questions
        - [H20GPT](https://github.com/h2oai/h2ogpt) [(hug)](https://huggingface.co/spaces/h2oai/h2ogpt-chatbot)
        - [StableLM-Tuned-Alpha](https://huggingface.co/spaces/stabilityai/stablelm-tuned-alpha-chat) [(github)](https://github.com/Stability-AI/StableLM)(datasets used: Stanford's Alpaca, Nomic-AI's gpt4all, RyokoAI's ShareGPT52K datasets, Databricks labs' Dolly, and Anthropic's HH. )
        - [OpenChatKit](https://github.com/togethercomputer/OpenChatKit) [(hug)](https://huggingface.co/spaces/togethercomputer/OpenChatKit) instruction-tuned language models, a moderation model, and an extensible retrieval system for including up-to-date responses from custom repositories
        - [Vicuna](https://github.com/lm-sys/FastChat)


- STEP 4 - Ask user for any clarification if question is not understood clearly
- STEP 5 - Check cache
    - Check from "done tasks" if similar task already done and relay the output
    - Check the saved knowledge graph for answers (personal contextual information)
- STEP 6 - Add task to queue
    - Check "current tasks" if any new task needs to be added or an old one needs refining, breaking down or prioritizing or need to be skipped
    - Choose a model (from list mentioned below), confirm with user
    - Push to task queue

- STEP 7 - Wait
    - If no task in the queue for a while, create suggestions, do them if allowed
    - Displays output from callbaks in the chat when recieved

### Background thread
- Execute tasks in separate virtual threads. Generate a callback to the Chat thread when done
- Save output in files and add knowledge to the knowledge graph

### Model types
- web search
    - reading webpages
    - code documentation search
    - reading blogs
    - reading code from github / stackoverflow

- image processing
    - image description [CLIP](https://github.com/openai/CLIP)
    - prompt generator for stable diffusion based on initial image
    - answer a question about an image [BLIP2](https://huggingface.co/spaces/Salesforce/BLIP2)
    - [Images to music](https://huggingface.co/spaces/fffiloni/img-to-music)
    - image editing
        - using natural language input [image editing](https://huggingface.co/spaces/microsoft/visual_chatgpt) or [instruct-pix2pix](https://huggingface.co/spaces/timbrooks/instruct-pix2pix)
        - generarte nearby image [stablediffusion-infinity](https://huggingface.co/spaces/lnyan/stablediffusion-infinity) or [diffuse-the-rest](https://huggingface.co/spaces/huggingface-projects/diffuse-the-rest)
        - [Mix images](https://huggingface.co/spaces/lambdalabs/image-mixer-demo)
    - generate an image [Stable Diffusion 2.0](https://huggingface.co/spaces/stabilityai/stable-diffusion) [(github)](https://github.com/Stability-AI/stablediffusion) in a finetuned style https://huggingface.co/spaces/anzorq/finetuned_diffusion with [diffusers](https://github.com/Stability-AI/diffusers) [(gallery)](https://huggingface.co/spaces/huggingface-projects/diffusers-gallery) controlled by [Controlnet](https://github.com/lllyasviel/ControlNet)[(Hug)](https://huggingface.co/spaces/hysts/ControlNet) (to control the level of convolution)
        - Dreamlike
            - [DreamShaper](https://huggingface.co/Lykon/DreamShaper)
            - midjourney iamges
                - [openjourney-v4](https://huggingface.co/prompthero/openjourney-v4)
                - [openjourney](https://huggingface.co/prompthero/openjourney)
            - [dreamlike-photoreal-2.0](https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0)
            - [realistic-skin-style](https://huggingface.co/shindi/realistic-skin-style)
            - [Analog-Diffusion](https://huggingface.co/wavymulder/Analog-Diffusion)
            - [blade-runner-2049-v1](https://huggingface.co/wimvanhenden/blade-runner-2049-v1)
            - [mo-di-diffusion](https://huggingface.co/nitrosocke/mo-di-diffusion)
            - [FeverDream](https://huggingface.co/Cosmo-Hug/FeverDream)
            - [glitched](https://huggingface.co/sd-dreambooth-library/glitched)
            - [naturitize-sd1-5-768px](https://huggingface.co/plasmo/naturitize-sd1-5-768px)
            - [Inkpunk-Diffusion](https://huggingface.co/Envvi/Inkpunk-Diffusion)
            - [VToonify](https://huggingface.co/spaces/PKUWilliamYang/VToonify)
            - [hassanblend1.4](https://huggingface.co/hassanblend/hassanblend1.4)
            - [lowpoly-world](https://huggingface.co/MirageML/lowpoly-world)
        - Replicating nature
            - [Stable_Diffusion_Microscopic_model](https://huggingface.co/Fictiverse/Stable_Diffusion_Microscopic_model)
            - [seacreatures](https://huggingface.co/arnomatic/seacreatures)
            - [JWST-Deep-Space-diffusion](https://huggingface.co/dallinmackay/JWST-Deep-Space-diffusion)


- video processing
    - [generation from text input](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis)
    - describe a video

- 3D processing
    - generation from text input [Point-e](https://huggingface.co/spaces/openai/point-e)
    - describe a 3D point cloud



## NOTES
- UI options:
    - https://github.com/h2oai/wave
    - https://github.com/Stability-AI/webui-stability-api
    - https://dreamstudio.ai/
    - https://huggingface.co/spaces/camenduru/webui

- [openai-cookbook](https://github.com/openai/openai-cookbook)
- [evals](https://github.com/openai/evals)

- [HuggingGPT](https://huggingface.co/spaces/microsoft/HuggingGPT)
- [Multilingual LLM](https://github.com/EleutherAI/polyglot )

- Talking face
    - [ml-talking-face](https://huggingface.co/spaces/CVPR/ml-talking-face)
    - [Chat-GPT-LangChain](https://huggingface.co/spaces/JavaFXpert/Chat-GPT-LangChain)

- Fix faces [GFPGAN](https://huggingface.co/spaces/Xintao/GFPGAN)
    - [CodeFormer](https://huggingface.co/spaces/sczhou/CodeFormer)

- [AnimeGANv2](https://huggingface.co/spaces/akhaliq/AnimeGANv2)
- [knollingcase-embeddings-sd-v2-0](https://huggingface.co/ProGamerGov/knollingcase-embeddings-sd-v2-0)
- [papercutcraft-v1](https://huggingface.co/OlafII/papercutcraft-v1)

- [stable-diffusion](https://github.com/CompVis/stable-diffusion) & [latent-diffusion](https://github.com/CompVis/latent-diffusion)
- [DALL-E](https://github.com/openai/DALL-E)
- [dalle-mini](https://github.com/borisdayma/dalle-mini) & [dalle-mini/dalle-mini](https://huggingface.co/spaces/dalle-mini/dalle-mini)

- [minigpt4](https://huggingface.co/spaces/Vision-CAIR/minigpt4)
- [GPT4ALL](https://github.com/nomic-ai/gpt4all) assistant-style large language model based on GPT-J and LLaMa
- [gpt-3](https://github.com/openai/gpt-3)
- [gpt-2](https://github.com/openai/gpt-2)
- [DialoGPT-medium](https://huggingface.co/microsoft/DialoGPT-medium)

- [Run LLaMA and Alpaca on your computer.](https://github.com/cocktailpeanut/dalai )
- [instruction-following LLaMA model](https://github.com/tatsu-lab/stanford_alpaca )
- [ Llama](https://github.com/facebookresearch/llama)
- [instruction-following large language model](https://github.com/databrickslabs/dolly)
(brainstorming, classification, closed QA, generation, information extraction, open QA and summarization.)
